{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Прочитать про методы оптимизации для нейронных сетей https://habr.com/post/318970/\n",
    "\n",
    " - Реализовать самостоятельно логистическую регрессию\n",
    "\n",
    "   - Обучить ее методом градиентного спуска\n",
    "   - Методом nesterov momentum\n",
    "   - Методом rmsprop\n",
    " - В качестве dataset’а взять Iris, оставив 2 класса:\n",
    "\n",
    "   - Iris Versicolor\n",
    "   - Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании был выбран более сложный алгоритм логистической регресии через функцию softmax, которая позволяет работать с произвольным количеством предсказываемых классов. Для каждого класса ведется свой набор коэффициентов, в результате имеем не (n,) подбираемых коофициентов, а матрицу (k,n) коэффициентов, k-тая строка описывает k-тый класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Параметры:\n",
    "    ----------\n",
    "      max_iter: количество итераций алгоритма градиентного спуска\n",
    "      solver: используемый алгоритм, возможные значения \"sgd\",\"nesterov\",\"rmsprop\"\n",
    "      eta: скорость обучения\n",
    "      gamma: гиперпараметр для агоритмов nesterov и rmsprop\n",
    "      epsilon: гиперпарметр алгоритма rmsprop\n",
    "      verbose: вывод отладочной информации при работе алгоритма\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=1000, solver=\"sgd\", eta=0.01, gamma=0.9, epsilon=10e-8, verbose=False):\n",
    "        self.max_iter = max_iter\n",
    "        if solver == \"sgd\":\n",
    "          self.solver = solver, None, None, self._sgd\n",
    "        elif solver == \"nesterov\":\n",
    "          self.solver = solver, self._nesterov_init, self._nesterov_get_theta_for_gradient_calc, self._nesterov\n",
    "        elif solver == \"rmsprop\":\n",
    "          self.solver = solver, self._rmsprop_init, None, self._rmsprop\n",
    "        else:\n",
    "          raise ValueError(\"Invalid solver '%s' specified.\" % solver)\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _sgd(self, theta, gradients):\n",
    "      # theta, gradients - (k,n)\n",
    "      return theta - self.eta * gradients\n",
    "\n",
    "    def _nesterov_init(self, k, n):\n",
    "      self.vt_prev = np.zeros((k, n))\n",
    "\n",
    "    def _nesterov_get_theta_for_gradient_calc(self, theta):\n",
    "      return theta - self.gamma * self.vt_prev\n",
    "      \n",
    "    def _nesterov(self, theta, gradients):\n",
    "      vt = self.gamma * self.vt_prev + self.eta * gradients\n",
    "      self.vt_prev = vt\n",
    "      return theta - vt\n",
    "\n",
    "    def _rmsprop_init(self, k, n):\n",
    "      self.EJ2_prev = np.zeros((k, n))\n",
    "\n",
    "    def _rmsprop(self, theta, gradients):\n",
    "      EJ2 = self.gamma * self.EJ2_prev + (1 - self.gamma) * gradients * gradients\n",
    "      self.EJ2_prev = EJ2\n",
    "      return theta - self.eta * gradients / np.sqrt(EJ2 + self.epsilon)\n",
    "\n",
    "    def _print(self, msg, *msg_args):\n",
    "        if not self.verbose:\n",
    "            return\n",
    "        if self.verbose < 50:\n",
    "            writer = sys.stderr.write\n",
    "        else:\n",
    "            writer = sys.stdout.write\n",
    "        writer(msg.format(*msg_args) + \"\\n\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "      assert len(X.shape) == 2 and len(y.shape) == 1\n",
    "      assert X.shape[0] == y.shape[0]\n",
    "\n",
    "      # M - кол-во образцов, N - кол-во признаков\n",
    "      M, N = X.shape\n",
    "\n",
    "      # K - количество классов\n",
    "      self.class_values = np.unique(y)\n",
    "      K = len(self.class_values)\n",
    "\n",
    "      # обнуляем временные переменные регуляризации если они нужны\n",
    "      if self.solver[1] is not None:\n",
    "        self.solver[1](K, N)\n",
    "\n",
    "      # подготавливаем (k,n) матрицу коэффициентов, каждая строка представляет коэффициенты k-го класса\n",
    "      theta = np.random.randn(K, N)\n",
    "\n",
    "      self._print(\"Старт обучения, solver '{}', theta:\\n{}\\n\", self.solver[0], theta)\n",
    "\n",
    "      # yk = 1 - если целевым классом для i-го образца является k, иначе 0 -> (k,m)\n",
    "      yk = np.array([[1 if y[m] == self.class_values[k] else 0 for m in range(M)] for k in range(K)])\n",
    "\n",
    "      # далее цикл градиентного спуска\n",
    "      for itertion in range(self.max_iter):\n",
    "\n",
    "        if self.solver[2] is not None:\n",
    "          theta = self.solver[2](theta)\n",
    "\n",
    "        #  Sk(x) = theta * X\n",
    "        # theta - (k,n), X = (m,n), skx = (k,m)\n",
    "        skx = np.dot(theta, X.T)\n",
    "\n",
    "        # exp(Sk(x)) -> (k,m)\n",
    "        eskx = np.exp(skx)\n",
    "\n",
    "        # sum(exp(Sk(x))) -> (m,)\n",
    "        sum_eskx = eskx.sum(axis=0)\n",
    "\n",
    "        # Pk = exp(Sk(x))/sum(exp(Sk(x))) -> (k,m)\n",
    "        pk = eskx / sum_eskx\n",
    "\n",
    "        # считаем векторы-градиенты перекрестной энтропии (функции издержек) для классов -> (k,n)\n",
    "        gradients = np.dot(pk - yk, X) / M\n",
    "\n",
    "        # движемся по гиперповерхности\n",
    "        theta = self.solver[3](theta, gradients)\n",
    "\n",
    "        if itertion % 100 == 0 :\n",
    "          self._print(\"Итерация {} theta:\\n{}\\n\", itertion, theta)\n",
    "\n",
    "      self.theta = theta\n",
    "      self._print(\"Конец обучения. theta:\\n{}\\n\", theta)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "      assert len(X.shape) == 2 and X.shape[1] == self.theta.shape[1]\n",
    "      \n",
    "      skx = np.dot(self.theta, X.T)        # -> (k,d)\n",
    "      eskx = np.exp(skx)          # -> (k,d)\n",
    "      sum_eskx = eskx.sum(axis=0) # -> (d,)\n",
    "      pk = eskx / sum_eskx        # -> (k,d)\n",
    "      return pk.T\n",
    "\n",
    "    def predict(self, X):\n",
    "      assert len(X.shape) == 2 and X.shape[1] == self.theta.shape[1]\n",
    "      # theta - (k,n), X - (d,n)  ->  (d,)\n",
    "\n",
    "      skx = np.dot(self.theta, X.T)        # -> (k,d)\n",
    "      y = np.argmax(skx, axis=0)  # -> (d,)\n",
    "      return [self.class_values[v] for v in y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X, y, feature_names, target_names = data.data, data.target, data.feature_names, data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  (\"Sequential Gradient Descent\", CustomLogisticRegression(solver=\"sgd\")),\n",
    "  (\"Nesterov momentum\", CustomLogisticRegression(solver=\"nesterov\")),\n",
    "  (\"Root Mean Square Propogation\", CustomLogisticRegression(solver=\"rmsprop\")),\n",
    "  (\"sklearn LogisticRegression\", LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\"))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  score_train = model.score(X_train, y_train)\n",
    "  score_test = model.score(X_val, y_val)\n",
    "  cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "  results.append((name, score_train, score_test, cm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Algorithm name: Sequential Gradient Descent\n",
      "Accuracy train: 0.8666666666666667\n",
      "Accuracy test: 0.825\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 0 36  1]\n",
      " [ 0 20 23]]\n",
      "------------------\n",
      "Algorithm name: Nesterov momentum\n",
      "Accuracy train: 1.0\n",
      "Accuracy test: 0.9583333333333334\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 0 33  4]\n",
      " [ 0  1 42]]\n",
      "------------------\n",
      "Algorithm name: Root Mean Square Propogation\n",
      "Accuracy train: 1.0\n",
      "Accuracy test: 0.9666666666666667\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 0 34  3]\n",
      " [ 0  1 42]]\n",
      "------------------\n",
      "Algorithm name: sklearn LogisticRegression\n",
      "Accuracy train: 0.9666666666666667\n",
      "Accuracy test: 0.9333333333333333\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 0 36  1]\n",
      " [ 0  7 36]]\n"
     ]
    }
   ],
   "source": [
    "for name, score_train, score_test, cm in results:\n",
    "  print(\"------------------\")\n",
    "  print(\"Algorithm name:\", name)\n",
    "  print(\"Accuracy train:\", score_train)\n",
    "  print(\"Accuracy test:\", score_test)\n",
    "  print(\"Confusion matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
